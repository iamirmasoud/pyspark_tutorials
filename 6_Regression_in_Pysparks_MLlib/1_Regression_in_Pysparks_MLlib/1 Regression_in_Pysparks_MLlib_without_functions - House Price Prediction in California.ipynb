{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression in PySpark's MLlib\n",
    "\n",
    "PySpark offers 7 algorithms for regression which we will review in this lecture. The content of this notebook will be very similar to what we did in the classification lectures where we will see how to prep our data first, and then go over how to train and evaluate each model individually. \n",
    "\n",
    "**Recap from the Regression lecture**<br>\n",
    "Remember that regression problems require that the **dependent variable** in your dataset be continuous (like age or height) and not categorical like (young vs old, or fat vs skinny). Regression analysis tries to find the relationship between this variable (the dependent) and each of the independent variables which can be either continuous or categorical. As with any machine learning problem, the basic question of regression analysis is \"what factors affect our outcome.\"\n",
    "\n",
    "For example, some research questions that might be solved using regression analysis might be:\n",
    "\n",
    "What factors effect...\n",
    "\n",
    "1. inflation rate and how we predict it longer term?\n",
    "2. price increases upon demand\n",
    "3. the height or weight of a person\n",
    "4. crop yield of vegetation like corn or apple trees\n",
    "5. income of a person\n",
    "\n",
    "\n",
    "## Available Algorithms\n",
    "These are the regression algorithms Spark offers:\n",
    "\n",
    "1. Linear regression \n",
    "     - most simplistic and easy to understand\n",
    "2. Generalized linear regression (out of scope)\n",
    "3. Decision tree regression \n",
    "     - most basic of the tree algorithms)\n",
    "4. Random forest regression \n",
    "     - a bit more complex than decision tree as it is an ensemble method as it combines several models in order to produce one really great predictive model\n",
    "5. Gradient-boosted tree regression \n",
    "     - most complex of the tree algorithms as it takes a more hierarchical approach to learning making it more efficient\n",
    "6. Survival regression (Out of scope)\n",
    "7. Isotonic regression (Out of scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 10:59:08 WARN Utils: Your hostname, masoud-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.7.139 instead (on interface wlp2s0)\n",
      "22/10/12 10:59:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 10:59:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.7.139:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Regression</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f21ef1b2bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark  # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Regression\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dependencies\n",
    "\n",
    "# For data prep\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import skewness, when, log, exp, col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# To check for multicolinearity\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# For training and evaluation\n",
    "from pyspark.ml.regression import (\n",
    "    LinearRegression,\n",
    "    DecisionTreeRegressor,\n",
    "    GBTRegressor,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "\n",
    "This is a dataset containing housing pricing data for California. Each row of data represents the median statistics for a block (eg. median income, median age of house, etc.). You could this data in a number of ways, but we will use it to predict the median house value. \n",
    "\n",
    "\n",
    "### About this dataset \n",
    "\n",
    "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "4. totalRooms: Total number of rooms within a block\n",
    "5. totalBedrooms: Total number of bedrooms within a block\n",
    "6. population: Total number of people residing within a block\n",
    "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "10. oceanProximity: Location of the house w.r.t ocean/sea\n",
    "\n",
    "**Source:** https://www.kaggle.com/camnugent/california-housing-prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"Datasets/\"\n",
    "df = spark.read.csv(path + \"housing.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>4.0368</td>\n",
       "      <td>269700.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "5    -122.25     37.85                52.0        919.0           213.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
       "5       413.0       193.0         4.0368            269700.0        NEAR BAY  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And of course the schema :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Starting\n",
    "print(df.count())\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can cut this dataframe down to a smaller size to get results faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to make your code run faster, you can slice the df like this...\n",
    "# # Slice rows\n",
    "# df = df.limit(300)\n",
    "\n",
    "# # Slice columns\n",
    "# cols_list = df.columns[4:9]\n",
    "# df = df.select(cols_list)\n",
    "\n",
    "# # QA\n",
    "# print(df.count())\n",
    "# print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop any missing values**\n",
    "\n",
    "Let's go ahead and drop any missing values for the sake of simplicity for this lecture as we have already covered the alternatives in subsequent lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20433"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop missing data\n",
    "df = df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data \n",
    "\n",
    "MLlib requires all input columns of your dataframe to be vectorized. You will see that we rename our dependent var to label as that is what is expected for all MLlib applications. If we rename once here, we won't need to specify it later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = [\"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "dependent_var = \"median_house_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = df.withColumnRenamed(dependent_var, \"label\")\n",
    "\n",
    "# Make sure dependent variable is numeric and change if it's not\n",
    "if renamed.schema[\"label\"].dataType != IntegerType():\n",
    "    renamed = renamed.withColumn(\"label\", renamed[\"label\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string type data in the input column list to numeric\n",
    "# Otherwise the Algorithm will not be able to process it\n",
    "\n",
    "# First create empty lists set up to divide you input list into numeric and string data types\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    if renamed.schema[column].dataType == StringType():\n",
    "        new_col_name = column + \"_num\"\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        numeric_inputs.append(column)\n",
    "        indexed = renamed\n",
    "\n",
    "# If the dataframe contains string types\n",
    "if len(string_inputs) != 0:\n",
    "    # Then use the string indexer to convert them to numeric\n",
    "    # Be careful not to convert a continuous variable that was read in incorrectly here\n",
    "    # This is meant for categorical columns only\n",
    "    for column in input_columns:\n",
    "        if renamed.schema[column].dataType == StringType():\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column + \"_num\")\n",
    "            indexed = indexer.fit(renamed).transform(renamed)\n",
    "else:\n",
    "    indexed = renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat outliers\n",
    "\n",
    "This is same approach as we discussed in the classification lecture. It is optional but may improve your model performance so it's considered best practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_bedrooms has been treated for positive (right) skewness. (skew =) 3.4592923587675024 )\n",
      "population has been treated for positive (right) skewness. (skew =) 4.959652416875933 )\n",
      "households has been treated for positive (right) skewness. (skew =) 3.4135995729616138 )\n",
      "median_income has been treated for positive (right) skewness. (skew =) 1.6444361858367003 )\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary d\n",
    "d = {}\n",
    "# Create a dictionary of percentiles you want to set\n",
    "# We do top and bottom 1 % which is pretty common\n",
    "for col in numeric_inputs:\n",
    "    d[col] = indexed.approxQuantile(\n",
    "        col, [0.01, 0.99], 0.25\n",
    "    )  # if you want to make it go faster increase the last number\n",
    "# Now fill in the values\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect()  # check for skewness\n",
    "    skew = skew[0][0]\n",
    "    # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
    "    if skew > 1:\n",
    "        indexed = indexed.withColumn(\n",
    "            col,\n",
    "            log(\n",
    "                when(df[col] < d[col][0], d[col][0])\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\n",
    "                .otherwise(indexed[col])\n",
    "                + 1\n",
    "            ).alias(col),\n",
    "        )\n",
    "        print(\n",
    "            col + \" has been treated for positive (right) skewness. (skew =)\", skew, \")\"\n",
    "        )\n",
    "    elif skew < -1:\n",
    "        indexed = indexed.withColumn(\n",
    "            col,\n",
    "            exp(\n",
    "                when(df[col] < d[col][0], d[col][0])\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\n",
    "                .otherwise(indexed[col])\n",
    "            ).alias(col),\n",
    "        )\n",
    "        print(\n",
    "            col + \" has been treated for negative (left) skewness. (skew =\", skew, \")\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[4.86753445045558...|452600.0|\n",
      "|[7.00940893270863...|358500.0|\n",
      "|[5.25227342804663...|352100.0|\n",
      "|[5.46383180502561...|341300.0|\n",
      "|[5.63835466933374...|342200.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list = numeric_inputs + string_inputs\n",
    "assembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")\n",
    "final_data = assembler.transform(indexed).select(\"features\", \"label\")\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Multicollinearity\n",
    "\n",
    "Multicollinearity generally occurs when there are high correlations between two or more predictor variables (your features column in your dataframe, also called independent variables). In other words, one predictor variable can be used to predict the other. This creates redundant information, skewing the results in a regression model. \n",
    "\n",
    "An easy way to detect multicollinearity is to calculate correlation coefficients for all pairs of predictor variables. If the correlation coefficient, is exactly +1 or -1, this is called perfect multicollinearity, and one of the variables should be removed from the model if at all possible for the linear model to perform well.\n",
    "\n",
    "Decision trees on the other hand, make no assumptions on relationships between features. It just constructs splits on single features that improves classification, based on an impurity measure like Gini or entropy. If features A, B are heavily correlated, no /little information can be gained from splitting on B after having split on A. So it would typically get ignored in favor of C.\n",
    "\n",
    "Of course a single decision tree is very vulnerable to overfitting, so one must either limit depth, prune heavily or preferly average many using an ensemble. Such problems get worse with many features and possibly also with co-variance but this problem occurs independently from multicolinearity.\n",
    "\n",
    "MLlib offers two correlation coefficient statistics: **pearson** and **spearman**. \n",
    "\n",
    "**Sources:**\n",
    "\n",
    " - https://datascience.stackexchange.com/questions/31402/multicollinearity-in-decision-tree\n",
    " - https://www.statisticshowto.datasciencecentral.com/multicollinearity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml.stat import Correlation\n",
    "pearsonCorr = Correlation.corr(final_data, \"features\", \"pearson\").collect()[0][0]\n",
    "array = pearsonCorr.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.89752321, 0.97459262, 0.0144038 ],\n",
       "       [0.89752321, 1.        , 0.93218972, 0.03178967],\n",
       "       [0.97459262, 0.93218972, 1.        , 0.04694401],\n",
       "       [0.0144038 , 0.03178967, 0.04694401, 1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the first and second features are highly correlated, along with the 4th and 5th and 4th and 6th. We may want to consider removing one of the variables in each correlation pair if we decide to use a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split dataframe into training and evaluation (test) dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Let's train a Random Forest algorithm to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 10:59:48 WARN Instrumentation: [7639f0e6] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Fit our model\n",
    "regressor = LinearRegression()\n",
    "fitModel = regressor.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "We will use the Root Mean Squared Error as our evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which evaluator you want to use\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 81319.1\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = fitModel.transform(test)\n",
    "# Select (prediction, true label) and compute test error\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (without cross validation)\n",
    "\n",
    "Linear regression is the most basic and commonly used predictive analysis. Regression estimates are used to describe data and to explain the relationship between one dependent variable and one or more independent variables.\n",
    "\n",
    "“Simple” linear regression means there is only one independent variable and “multiple” linear regression means there is more than one independent variables. \n",
    "\n",
    "Researchers often use a linear relationship to predict the (average) numerical value of Y for a given value of X using a straight line (called the regression line). If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y. In other words, you predict (the average) Y from X. Of course this is just for simple linear regression. As you add more variables, your graph will look more and more complex as it becomes a three-dimensional space. But this is the basic concept. Think back to your high school algebra class :) Your goal with this analysis is to find the line that has the least amount of error which is the sum of the distance between each one of the data points and the line.\n",
    "\n",
    "**PySpark Documentation Link:** https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 11:10:48 WARN Instrumentation: [d50c838d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "\u001b[1mLinear Regression Model Summary without cross validation:\u001b[0m\n",
      " \n",
      "Intercept: -15184.654528954561\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|feature       |coeff              |\n",
      "+--------------+-------------------+\n",
      "|median_income |212188.83658773726 |\n",
      "|households    |111742.33672058037 |\n",
      "|total_bedrooms|-9938.397185760417 |\n",
      "|population    |-100825.73550065797|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "numIterations: 0\n",
      "objectiveHistory: (scaled loss + regularization) at each iteration \n",
      " [0.0]\n",
      "\n",
      "Training RMSE: 81516.736023\n",
      "Training r2: 0.502751\n",
      "\n",
      "Test RMSE: 81319.12283604541\n",
      "Test r2: 0.5002961450994672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "# first without cross val\n",
    "fitModel = regressor.fit(train)\n",
    "\n",
    "# Load the Summary\n",
    "trainingSummary = fitModel.summary\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\n",
    "    \"\\033[1m\" + \"Linear Regression Model Summary without cross validation:\" + \"\\033[0m\"\n",
    ")\n",
    "print(\" \")\n",
    "print(\"Intercept: %s\" % str(fitModel.intercept))\n",
    "print(\"\")\n",
    "coeff_array = fitModel.coefficients.toArray()\n",
    "coeff_scores = []\n",
    "for x in coeff_array:\n",
    "    coeff_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, coeff_scores), schema=[\"feature\", \"coeff\"]\n",
    ")\n",
    "print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\n",
    "    \"objectiveHistory: (scaled loss + regularization) at each iteration \\n %s\"\n",
    "    % str(trainingSummary.objectiveHistory)\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# Print the Errors\n",
    "print(\"Training RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"Training r2: %f\" % trainingSummary.r2)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Now load the test results\n",
    "test_results = fitModel.evaluate(test)\n",
    "\n",
    "# And print them\n",
    "print(\"Test RMSE: {}\".format(test_results.rootMeanSquaredError))\n",
    "print(\"Test r2: {}\".format(test_results.r2))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Model Summary WITH cross validation:\u001b[0m\n",
      " \n",
      "Coefficient Standard Errors: \n",
      "+--------------+------------------+\n",
      "|feature       |coeff std error   |\n",
      "+--------------+------------------+\n",
      "|households    |5142.39706243724  |\n",
      "|total_bedrooms|4202.4183707371985|\n",
      "|population    |2581.5634351539734|\n",
      "|median_income |1923.6974099341724|\n",
      "+--------------+------------------+\n",
      "\n",
      "None\n",
      " \n",
      "P Values: \n",
      "+--------------+--------------------+\n",
      "|feature       |P-Value             |\n",
      "+--------------+--------------------+\n",
      "|total_bedrooms|0.018074175404535042|\n",
      "|households    |0.0                 |\n",
      "|population    |0.0                 |\n",
      "|median_income |0.0                 |\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "RMSE: 81319.12000925008\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "# Now train with cross val\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.maxIter, [10, 15])\n",
    "    .addGrid(regressor.regParam, [0.1, 0.01])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=regressor, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2\n",
    ")  # 3 is best practice\n",
    "\n",
    "print(\"\\033[1m\" + \"Linear Regression Model Summary WITH cross validation:\" + \"\\033[0m\")\n",
    "print(\" \")\n",
    "# Run cross validations\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Extract Best model\n",
    "LR_BestModel = fitModel.bestModel\n",
    "\n",
    "# Get Model Summary Statistics\n",
    "# ModelSummary = fitModel.bestModel.summary\n",
    "ModelSummary = LR_BestModel.summary\n",
    "print(\"Coefficient Standard Errors: \")\n",
    "coeff_ste = ModelSummary.coefficientStandardErrors\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, coeff_ste), schema=[\"feature\", \"coeff std error\"]\n",
    ")\n",
    "print(result.orderBy(result[\"coeff std error\"].desc()).show(truncate=False))\n",
    "print(\" \")\n",
    "print(\"P Values: \")\n",
    "# Then zip with input_columns list and create a df\n",
    "pvalues = ModelSummary.pValues\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, pvalues), schema=[\"feature\", \"P-Value\"]\n",
    ")\n",
    "print(result.orderBy(result[\"P-Value\"].desc()).show(truncate=False))\n",
    "print(\" \")\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "ModelPredictions = fitModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "test_results = evaluator.evaluate(ModelPredictions)\n",
    "print(\"RMSE:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "\n",
    "Decision Trees break down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n",
    "\n",
    "Decision trees are most commonly used in classification exercises, but they can also used for regression problems as well. In a standard classification tree, the idea is to split the dataset based on homogeneity of data. Let's say for example we have two variables: age and weight that predict if a person is going to sign up for a gym membership or not. In our training data if it showed that 90% of the people who are older than 40 signed up, we split the data here and age becomes a top node in the tree. We can almost say that this split has made the data \"90% pure\". \n",
    "\n",
    "In a regression tree the idea is this: since the target variable does not have classes, we fit a regression model to the target variable using each of the independent variables. Then for each independent variable, the data is split at several split points. At each split point, the \"error\" between the predicted value and the actual values is squared to get a \"Sum of Squared Errors (SSE)\". The split point errors across the variables are compared and the variable/point yielding the lowest SSE is chosen as the root node/split point. This process is recursively continued.\n",
    "\n",
    "The core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking. The ID3 algorithm can be used to construct a decision tree for regression by replacing Information Gain with Standard Deviation Reduction.\n",
    "\n",
    "\n",
    "**PySpark Documentation Link:** https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 11:25:28 WARN DAGScheduler: Broadcasting large task binary with size 1003.2 KiB\n",
      "22/10/12 11:25:28 WARN DAGScheduler: Broadcasting large task binary with size 1092.3 KiB\n",
      "22/10/12 11:25:28 WARN DAGScheduler: Broadcasting large task binary with size 1159.0 KiB\n",
      "22/10/12 11:25:29 WARN DAGScheduler: Broadcasting large task binary with size 1209.0 KiB\n",
      "22/10/12 11:25:30 WARN DAGScheduler: Broadcasting large task binary with size 1095.8 KiB\n",
      "22/10/12 11:25:30 WARN DAGScheduler: Broadcasting large task binary with size 1254.0 KiB\n",
      "22/10/12 11:25:30 WARN DAGScheduler: Broadcasting large task binary with size 1394.4 KiB\n",
      "22/10/12 11:25:30 WARN DAGScheduler: Broadcasting large task binary with size 1506.2 KiB\n",
      "22/10/12 11:25:30 WARN DAGScheduler: Broadcasting large task binary with size 1585.5 KiB\n",
      "22/10/12 11:25:31 WARN DAGScheduler: Broadcasting large task binary with size 1014.1 KiB\n",
      "22/10/12 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 1003.2 KiB\n",
      "22/10/12 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 1092.3 KiB\n",
      "22/10/12 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 1159.0 KiB\n",
      "22/10/12 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 1209.0 KiB\n",
      "22/10/12 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 1238.4 KiB\n",
      "22/10/12 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 1257.5 KiB\n",
      "22/10/12 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 1269.4 KiB\n",
      "22/10/12 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 1277.2 KiB\n",
      "22/10/12 11:25:34 WARN DAGScheduler: Broadcasting large task binary with size 1280.8 KiB\n",
      "22/10/12 11:25:34 WARN DAGScheduler: Broadcasting large task binary with size 1282.7 KiB\n",
      "22/10/12 11:25:34 WARN DAGScheduler: Broadcasting large task binary with size 1283.7 KiB\n",
      "22/10/12 11:25:34 WARN DAGScheduler: Broadcasting large task binary with size 1284.3 KiB\n",
      "22/10/12 11:25:35 WARN DAGScheduler: Broadcasting large task binary with size 1095.8 KiB\n",
      "22/10/12 11:25:35 WARN DAGScheduler: Broadcasting large task binary with size 1254.0 KiB\n",
      "22/10/12 11:25:35 WARN DAGScheduler: Broadcasting large task binary with size 1394.4 KiB\n",
      "22/10/12 11:25:35 WARN DAGScheduler: Broadcasting large task binary with size 1506.2 KiB\n",
      "22/10/12 11:25:36 WARN DAGScheduler: Broadcasting large task binary with size 1585.5 KiB\n",
      "22/10/12 11:25:36 WARN DAGScheduler: Broadcasting large task binary with size 1637.9 KiB\n",
      "22/10/12 11:25:36 WARN DAGScheduler: Broadcasting large task binary with size 1671.4 KiB\n",
      "22/10/12 11:25:36 WARN DAGScheduler: Broadcasting large task binary with size 1693.9 KiB\n",
      "22/10/12 11:25:36 WARN DAGScheduler: Broadcasting large task binary with size 1708.4 KiB\n",
      "22/10/12 11:25:36 WARN DAGScheduler: Broadcasting large task binary with size 1715.1 KiB\n",
      "22/10/12 11:25:36 WARN DAGScheduler: Broadcasting large task binary with size 1719.8 KiB\n",
      "22/10/12 11:25:36 WARN DAGScheduler: Broadcasting large task binary with size 1723.4 KiB\n",
      "22/10/12 11:25:37 WARN DAGScheduler: Broadcasting large task binary with size 1726.4 KiB\n",
      "22/10/12 11:25:37 WARN DAGScheduler: Broadcasting large task binary with size 1728.2 KiB\n",
      "22/10/12 11:25:37 WARN DAGScheduler: Broadcasting large task binary with size 1071.0 KiB\n",
      "22/10/12 11:25:43 WARN DAGScheduler: Broadcasting large task binary with size 1054.4 KiB\n",
      "22/10/12 11:25:43 WARN DAGScheduler: Broadcasting large task binary with size 1137.9 KiB\n",
      "22/10/12 11:25:43 WARN DAGScheduler: Broadcasting large task binary with size 1197.5 KiB\n",
      "22/10/12 11:25:43 WARN DAGScheduler: Broadcasting large task binary with size 1234.2 KiB\n",
      "22/10/12 11:25:45 WARN DAGScheduler: Broadcasting large task binary with size 1043.0 KiB\n",
      "22/10/12 11:25:45 WARN DAGScheduler: Broadcasting large task binary with size 1227.1 KiB\n",
      "22/10/12 11:25:45 WARN DAGScheduler: Broadcasting large task binary with size 1386.2 KiB\n",
      "22/10/12 11:25:45 WARN DAGScheduler: Broadcasting large task binary with size 1501.3 KiB\n",
      "22/10/12 11:25:45 WARN DAGScheduler: Broadcasting large task binary with size 1588.7 KiB\n",
      "22/10/12 11:25:45 WARN DAGScheduler: Broadcasting large task binary with size 1023.3 KiB\n",
      "22/10/12 11:25:47 WARN DAGScheduler: Broadcasting large task binary with size 1054.4 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1137.9 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1197.5 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1234.2 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1256.8 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1270.1 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1278.8 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1286.3 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1292.8 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1296.6 KiB\n",
      "22/10/12 11:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1298.7 KiB\n",
      "22/10/12 11:25:49 WARN DAGScheduler: Broadcasting large task binary with size 1299.5 KiB\n",
      "22/10/12 11:25:49 WARN DAGScheduler: Broadcasting large task binary with size 1299.8 KiB\n",
      "22/10/12 11:25:49 WARN DAGScheduler: Broadcasting large task binary with size 1300.1 KiB\n",
      "22/10/12 11:25:50 WARN DAGScheduler: Broadcasting large task binary with size 1043.0 KiB\n",
      "22/10/12 11:25:50 WARN DAGScheduler: Broadcasting large task binary with size 1227.1 KiB\n",
      "22/10/12 11:25:50 WARN DAGScheduler: Broadcasting large task binary with size 1386.2 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1501.3 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1588.7 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1653.4 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1694.4 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1726.3 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1742.2 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1754.7 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1762.6 KiB\n",
      "22/10/12 11:25:51 WARN DAGScheduler: Broadcasting large task binary with size 1767.8 KiB\n",
      "22/10/12 11:25:52 WARN DAGScheduler: Broadcasting large task binary with size 1770.6 KiB\n",
      "22/10/12 11:25:52 WARN DAGScheduler: Broadcasting large task binary with size 1772.1 KiB\n",
      "22/10/12 11:25:52 WARN DAGScheduler: Broadcasting large task binary with size 1772.9 KiB\n",
      "22/10/12 11:25:52 WARN DAGScheduler: Broadcasting large task binary with size 1097.4 KiB\n",
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+--------------------+\n",
      "|feature       |score               |\n",
      "+--------------+--------------------+\n",
      "|median_income |0.958971982510569   |\n",
      "|population    |0.017881207535763576|\n",
      "|total_bedrooms|0.011991127016306875|\n",
      "|households    |0.011155682937360543|\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      "RMSE: 82280.67635327157\n"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor()\n",
    "\n",
    "# Build your parameter grid\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30])\n",
    "    .addGrid(regressor.maxBins, [10, 20, 40])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=regressor,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
    "    numFolds=2,\n",
    ")  # 3 is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "DT_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees\n",
    "# in the ensemble The importance vector is normalized to sum to 1.\n",
    "print(\" \")\n",
    "print(\"\\033[1m\" + \" Feature Importances\" + \"\\033[0m\")\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "DT_featureImportances = DT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in DT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, imp_scores), schema=[\"feature\", \"score\"]\n",
    ")\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "Random Forest is an ensemble method that predicts based on the majority of votes from each of the decision trees made. The idea is still the same here (with regression), it’s just that Standard Deviation Reduction techniques are used instead of Information Gain as we touched on in the Decision Tree explanation. \n",
    "\n",
    "**PySpark Documentation link:** https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/masoud/anaconda3/envs/spark_env/lib/python3.7/site-packages/pyspark/jars/spark-core_2.12-3.3.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 11:26:03 WARN DAGScheduler: Broadcasting large task binary with size 1008.3 KiB\n",
      "22/10/12 11:26:03 WARN DAGScheduler: Broadcasting large task binary with size 1495.2 KiB\n",
      "22/10/12 11:26:05 WARN DAGScheduler: Broadcasting large task binary with size 1164.6 KiB\n",
      "22/10/12 11:26:06 WARN DAGScheduler: Broadcasting large task binary with size 1861.2 KiB\n",
      "22/10/12 11:26:15 WARN DAGScheduler: Broadcasting large task binary with size 1099.7 KiB\n",
      "22/10/12 11:26:15 WARN DAGScheduler: Broadcasting large task binary with size 1644.7 KiB\n",
      "22/10/12 11:26:17 WARN DAGScheduler: Broadcasting large task binary with size 1199.9 KiB\n",
      "22/10/12 11:26:18 WARN DAGScheduler: Broadcasting large task binary with size 1908.4 KiB\n",
      "22/10/12 11:26:20 WARN DAGScheduler: Broadcasting large task binary with size 1233.3 KiB\n",
      "22/10/12 11:26:20 WARN DAGScheduler: Broadcasting large task binary with size 2030.7 KiB\n",
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+--------------------+\n",
      "|feature       |score               |\n",
      "+--------------+--------------------+\n",
      "|median_income |0.7739182031491099  |\n",
      "|population    |0.11449230907725716 |\n",
      "|households    |0.05790776908343548 |\n",
      "|total_bedrooms|0.053681718690197465|\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      "RMSE: 75299.05204981378\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "    .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "    .addGrid(regressor.numTrees, [5, 20])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=regressor, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2\n",
    ")  # 3 is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "RF_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees\n",
    "# in the ensemble The importance vector is normalized to sum to 1.\n",
    "print(\" \")\n",
    "print(\"\\033[1m\" + \" Feature Importances\" + \"\\033[0m\")\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "RF_featureImportances = RF_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in RF_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, imp_scores), schema=[\"feature\", \"score\"]\n",
    ")\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Regressor\n",
    "\n",
    "**Recap from the content review lecture** <br>\n",
    "The same goes with gradient boosting (Standard Deviation Reduction techniques are used instead of Information Gain), but with more of a hierarchical approach. It combines the weak learners (binary splits) to strong prediction rules that allow a flexible partition of the feature space. The objective here, as is of any supervised learning algorithm, is to define a loss function and minimize it. So it basically tries a whole bunch of different ways to analyze your data and returns the best one. \n",
    "\n",
    "**PySpark Documentation link:** https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3920:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1013.0 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1041.6 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1024.0 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1024.5 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1025.1 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1026.2 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1028.5 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1032.6 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1039.3 KiB\n",
      "22/10/12 11:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1048.9 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1060.3 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1074.8 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1064.0 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1064.4 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1065.0 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1066.1 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1068.4 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1072.8 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1081.3 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1095.8 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1118.0 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1146.4 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1129.8 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1130.3 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1130.9 KiB\n",
      "22/10/12 11:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1132.0 KiB\n",
      "22/10/12 11:26:58 WARN DAGScheduler: Broadcasting large task binary with size 1133.9 KiB\n",
      "22/10/12 11:26:58 WARN DAGScheduler: Broadcasting large task binary with size 1136.7 KiB\n",
      "22/10/12 11:26:58 WARN DAGScheduler: Broadcasting large task binary with size 1141.8 KiB\n",
      "22/10/12 11:26:58 WARN DAGScheduler: Broadcasting large task binary with size 1151.3 KiB\n",
      "22/10/12 11:26:58 WARN DAGScheduler: Broadcasting large task binary with size 1163.8 KiB\n",
      "22/10/12 11:26:58 WARN DAGScheduler: Broadcasting large task binary with size 1182.0 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1003.9 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1038.3 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1025.1 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1025.6 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1026.2 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1027.3 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1029.6 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1034.0 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1041.6 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1054.6 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1076.9 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1112.7 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1106.2 KiB\n",
      "22/10/12 11:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1107.3 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1108.4 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1110.7 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1115.1 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1123.8 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1138.2 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1161.5 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1195.5 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1182.2 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1182.6 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1183.2 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1184.3 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1186.6 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1190.8 KiB\n",
      "22/10/12 11:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1197.5 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1208.8 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1226.1 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1253.4 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1247.5 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1247.9 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1248.5 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1249.7 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1251.9 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1255.6 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1262.0 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1271.7 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1283.8 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1296.3 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1284.2 KiB\n",
      "22/10/12 11:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1284.7 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1285.2 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1286.4 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1288.6 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1293.1 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1300.6 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1312.9 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1329.6 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1354.4 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1346.4 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1346.9 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1347.4 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1348.6 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1350.8 KiB\n",
      "22/10/12 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1355.3 KiB\n",
      "22/10/12 11:27:09 WARN DAGScheduler: Broadcasting large task binary with size 1363.7 KiB\n",
      "22/10/12 11:27:09 WARN DAGScheduler: Broadcasting large task binary with size 1377.6 KiB\n",
      "22/10/12 11:27:09 WARN DAGScheduler: Broadcasting large task binary with size 1397.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 11:27:09 WARN DAGScheduler: Broadcasting large task binary with size 1427.6 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1012.6 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1002.4 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1002.9 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1003.4 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1004.5 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1006.8 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1011.2 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1019.9 KiB\n",
      "22/10/12 11:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1036.2 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1061.3 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1095.8 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1080.0 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1080.5 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1081.0 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1082.2 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1084.4 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1088.9 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1097.1 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1112.2 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1137.7 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1176.8 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1162.5 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1163.0 KiB\n",
      "22/10/12 11:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1163.6 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1164.7 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1166.9 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1170.8 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1177.4 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1188.2 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1203.6 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1225.0 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1214.0 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1214.5 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1215.1 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1216.2 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1218.5 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1222.9 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1231.3 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1246.9 KiB\n",
      "22/10/12 11:27:44 WARN DAGScheduler: Broadcasting large task binary with size 1270.9 KiB\n",
      "22/10/12 11:27:45 WARN DAGScheduler: Broadcasting large task binary with size 1307.1 KiB\n",
      "22/10/12 11:27:51 WARN DAGScheduler: Broadcasting large task binary with size 1003.2 KiB\n",
      "22/10/12 11:27:51 WARN DAGScheduler: Broadcasting large task binary with size 1019.5 KiB\n",
      "22/10/12 11:27:51 WARN DAGScheduler: Broadcasting large task binary with size 1047.1 KiB\n",
      "22/10/12 11:27:51 WARN DAGScheduler: Broadcasting large task binary with size 1089.3 KiB\n",
      "22/10/12 11:27:51 WARN DAGScheduler: Broadcasting large task binary with size 1079.2 KiB\n",
      "22/10/12 11:27:51 WARN DAGScheduler: Broadcasting large task binary with size 1079.7 KiB\n",
      "22/10/12 11:27:51 WARN DAGScheduler: Broadcasting large task binary with size 1080.3 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1081.4 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1083.6 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1087.5 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1094.2 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1105.9 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1125.2 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1154.0 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1144.4 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1144.9 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1145.5 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1146.6 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1148.9 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1153.4 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1161.6 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1175.7 KiB\n",
      "22/10/12 11:27:52 WARN DAGScheduler: Broadcasting large task binary with size 1199.9 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1238.2 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1228.6 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1229.1 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1229.7 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1230.8 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1233.1 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1237.3 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1244.6 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1256.5 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1275.9 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1307.0 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1299.5 KiB\n",
      "22/10/12 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 1299.9 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1300.5 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1301.7 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1303.9 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1308.4 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1317.2 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1332.9 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1355.6 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1384.5 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1368.4 KiB\n",
      "22/10/12 11:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1368.9 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1369.5 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1370.6 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1372.9 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1377.4 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1385.9 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1400.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1423.6 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1458.8 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1448.8 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1449.3 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1449.8 KiB\n",
      "22/10/12 11:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1451.0 KiB\n",
      "22/10/12 11:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1453.2 KiB\n",
      "22/10/12 11:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1457.8 KiB\n",
      "22/10/12 11:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1466.2 KiB\n",
      "22/10/12 11:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1478.1 KiB\n",
      "22/10/12 11:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1494.9 KiB\n",
      "22/10/12 11:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1519.8 KiB\n",
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+-------------------+\n",
      "|feature       |score              |\n",
      "+--------------+-------------------+\n",
      "|median_income |0.4524784292210871 |\n",
      "|population    |0.25447186826391793|\n",
      "|households    |0.1977405314488648 |\n",
      "|total_bedrooms|0.09530917106613003|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "RMSE: 75463.48338779979\n"
     ]
    }
   ],
   "source": [
    "regressor = GBTRegressor()\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "    .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=regressor, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2\n",
    ")  # 3 is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "GBT_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees\n",
    "# in the ensemble The importance vector is normalized to sum to 1.\n",
    "print(\" \")\n",
    "print(\"\\033[1m\" + \" Feature Importances\" + \"\\033[0m\")\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "GBT_featureImportances = GBT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in GBT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, imp_scores), schema=[\"feature\", \"score\"]\n",
    ")\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take aways\n",
    "\n",
    "We can see from the output above that the GBTRegressor way out performed the other models judging from the RMSE (compare Training RMSE to all others). No surprise there. \n",
    "\n",
    "We also notice that the most of the P Values for the linear regression model look pretty strong except for the second and third variables which we should consider removing if we go ahead with linear regression model. \n",
    "\n",
    "As for the tree models, it looks like population was the best feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how your predictions were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+\n",
      "|            features|   label|        prediction|\n",
      "+--------------------+--------+------------------+\n",
      "|[0.69314718055994...|500001.0|439283.64296953287|\n",
      "|[1.38629436111989...|275000.0|113830.01036131583|\n",
      "|[1.38629436111989...|250000.0|223003.89363902845|\n",
      "|[1.79175946922805...|162500.0|113830.01036131583|\n",
      "|[1.94591014905531...| 67500.0| 152375.3211684852|\n",
      "|[2.07944154167983...|500001.0|152902.87442141358|\n",
      "|[2.19722457733621...|225000.0| 371257.5625499525|\n",
      "|[2.30258509299404...|225000.0| 332859.5683332743|\n",
      "|[2.30258509299404...|112500.0|113830.01036131583|\n",
      "|[2.39789527279837...|193800.0| 371257.5625499525|\n",
      "|[2.48490664978800...|162500.0|154292.34038040356|\n",
      "|[2.48490664978800...| 67500.0|192095.32839109338|\n",
      "|[2.48490664978800...|187500.0|154292.34038040356|\n",
      "|[2.48490664978800...|112500.0|154292.34038040356|\n",
      "|[2.48490664978800...|500001.0|154292.34038040356|\n",
      "|[2.56494935746153...| 75000.0|192095.32839109338|\n",
      "|[2.70805020110221...|500001.0|117776.30529546743|\n",
      "|[2.70805020110221...| 87500.0|  224337.539470047|\n",
      "|[2.77258872223978...|141700.0|  224337.539470047|\n",
      "|[2.83321334405621...|350000.0| 371257.5625499525|\n",
      "+--------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = GBT_BestModel.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save your model you can simply..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# this path will create the folder\n",
    "path = \"Models/GBT_Model\"\n",
    "GBT_BestModel.save(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
