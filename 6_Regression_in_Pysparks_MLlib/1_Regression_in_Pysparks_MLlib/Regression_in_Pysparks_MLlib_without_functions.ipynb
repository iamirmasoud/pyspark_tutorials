{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression in PySpark's MLlib\n",
    "\n",
    "PySpark offers 7 algorithms for regression which we will review in this lecture. The content of this notebook will be very similar to what we did in the classification lectures where we will see how to prep our data first, and then go over how to train and evaluate each model individually. \n",
    "\n",
    "**Recap from the Regression lecture**<br>\n",
    "Remember that regression problems require that the **dependent variable** in your dataset be continuous (like age or height) and not categorical like (young vs old, or fat vs skinny). Regression analysis tries to find the relationship between this variable (the dependent) and each of the independent variables which can be either continous or categorical. As with any machine learning problem, the basic question of regression analysis is \"what factors affect our outcome.\"\n",
    "\n",
    "For example, some research questions that might be solved using regression anlaysis might be:\n",
    "\n",
    "What factors effect...\n",
    "\n",
    "1. inflation rate and how we predict it longer term?\n",
    "2. price increases upon demand\n",
    "3. the height or weight of a person\n",
    "4. crop yield of vegetation like corn or apple trees\n",
    "5. income of a person\n",
    "\n",
    "\n",
    "## Available Algorithms\n",
    "These are the regression algorithms Spark offers:\n",
    "\n",
    "1. Linear regression \n",
    "     - most simplistic and easy to understand\n",
    "2. Generalized linear regression (out of scope)\n",
    "3. Decision tree regression \n",
    "     - most basic of the tree algorithms)\n",
    "4. Random forest regression \n",
    "     - a bit more complex than decision tree as it is an ensemble method as it combines several models in order to produce one really great predictive model\n",
    "5. Gradient-boosted tree regression \n",
    "     - most complex of the tree algorithms as it takes a more hierarchical approach to learning making it more efficient\n",
    "6. Survival regression (Out of scope)\n",
    "7. Isotonic regression (Out of scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Regression</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112f313d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark  # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Regression\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dependencies\n",
    "\n",
    "# For data prep\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# To check for multicolinearity\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# For training and evaluation\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "\n",
    "This is a dataset containing housing pricing data for California. Each row of data represents the median statistics for a block (eg. median income, median age of house, etc.). You could this data in a number of ways, but we will use it to predict the median house value. \n",
    "\n",
    "\n",
    "### About this dataset \n",
    "\n",
    "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "4. totalRooms: Total number of rooms within a block\n",
    "5. totalBedrooms: Total number of bedrooms within a block\n",
    "6. population: Total number of people residing within a block\n",
    "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "10. oceanProximity: Location of the house w.r.t ocean/sea\n",
    "\n",
    "**Source:** https://www.kaggle.com/camnugent/california-housing-prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Datasets/\"\n",
    "df = spark.read.csv(path + \"housing.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>4.0368</td>\n",
       "      <td>269700.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "5    -122.25     37.85                52.0        919.0           213.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
       "5       413.0       193.0         4.0368            269700.0        NEAR BAY  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And of course the schema :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Starting\n",
    "print(df.count())\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can cut this dataframe down to a smaller size to get results faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to make your code run faster, you can slice the df like this...\n",
    "# # Slice rows\n",
    "# df = df.limit(300)\n",
    "\n",
    "# # Slice columns\n",
    "# cols_list = df.columns[4:9]\n",
    "# df = df.select(cols_list)\n",
    "\n",
    "# # QA\n",
    "# print(df.count())\n",
    "# print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop any missing values**\n",
    "\n",
    "Let's go ahead and drop any missing values for the sake of simplicity for this lecture as we have already covered the alternatives in subsequent lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20433"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop missing data\n",
    "df = df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data \n",
    "\n",
    "MLlib requires all input columns of your dataframe to be vectorized. You will see that we rename our dependent var to label as that is what is expected for all MLlib applications. If we rename once here, we won't need to specify it later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = [\"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "dependent_var = \"median_house_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = df.withColumnRenamed(dependent_var, \"label\")\n",
    "\n",
    "# Make sure dependent variable is numeric and change if it's not\n",
    "if str(renamed.schema[\"label\"].dataType) != \"IntegerType\":\n",
    "    renamed = renamed.withColumn(\"label\", renamed[\"label\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string type data in the input column list to numeric\n",
    "# Otherwise the Algorithm will not be able to process it\n",
    "\n",
    "# First create empty lists set up to divide you input list into numeric and string data types\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    if str(renamed.schema[column].dataType) == \"StringType\":\n",
    "        new_col_name = column + \"_num\"\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        numeric_inputs.append(column)\n",
    "        indexed = renamed\n",
    "\n",
    "# If the dataframe contains string types\n",
    "if len(string_inputs) != 0:\n",
    "    # Then use the string indexer to convert them to numeric\n",
    "    # Be careful not to convert a continuous variable that was read in incorrectly here\n",
    "    # This is meant for categorical columns only\n",
    "    for column in input_columns:\n",
    "        if str(renamed.schema[column].dataType) == \"StringType\":\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column + \"_num\")\n",
    "            indexed = indexer.fit(renamed).transform(renamed)\n",
    "else:\n",
    "    indexed = renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat outliers\n",
    "\n",
    "This is same approach as we discussed in the classification lecture. It is optional but may improve your model performance so it's considered best practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_bedrooms has been treated for positive (right) skewness. (skew =) 3.4592923587675024 )\n",
      "population has been treated for positive (right) skewness. (skew =) 4.959652416875933 )\n",
      "households has been treated for positive (right) skewness. (skew =) 3.4135995729616138 )\n",
      "median_income has been treated for positive (right) skewness. (skew =) 1.6444361858367003 )\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary d\n",
    "d = {}\n",
    "# Create a dictionary of percentiles you want to set\n",
    "# We do top and bottom 1 % which is pretty common\n",
    "for col in numeric_inputs:\n",
    "    d[col] = indexed.approxQuantile(\n",
    "        col, [0.01, 0.99], 0.25\n",
    "    )  # if you want to make it go faster increase the last number\n",
    "# Now fill in the values\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect()  # check for skewness\n",
    "    skew = skew[0][0]\n",
    "    # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
    "    if skew > 1:\n",
    "        indexed = indexed.withColumn(\n",
    "            col,\n",
    "            log(\n",
    "                when(df[col] < d[col][0], d[col][0])\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\n",
    "                .otherwise(indexed[col])\n",
    "                + 1\n",
    "            ).alias(col),\n",
    "        )\n",
    "        print(\n",
    "            col + \" has been treated for positive (right) skewness. (skew =)\", skew, \")\"\n",
    "        )\n",
    "    elif skew < -1:\n",
    "        indexed = indexed.withColumn(\n",
    "            col,\n",
    "            exp(\n",
    "                when(df[col] < d[col][0], d[col][0])\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\n",
    "                .otherwise(indexed[col])\n",
    "            ).alias(col),\n",
    "        )\n",
    "        print(\n",
    "            col + \" has been treated for negative (left) skewness. (skew =\", skew, \")\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[4.86753445045558...|452600.0|\n",
      "|[7.00940893270863...|358500.0|\n",
      "|[5.25227342804663...|352100.0|\n",
      "|[5.46383180502561...|341300.0|\n",
      "|[5.63835466933374...|342200.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list = numeric_inputs + string_inputs\n",
    "assembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")\n",
    "final_data = assembler.transform(indexed).select(\"features\", \"label\")\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Multicollinearity\n",
    "\n",
    "Multicollinearity generally occurs when there are high correlations between two or more predictor variables (your features column in your dataframe, also called independent variables). In other words, one predictor variable can be used to predict the other. This creates redundant information, skewing the results in a regression model. \n",
    "\n",
    "An easy way to detect multicollinearity is to calculate correlation coefficients for all pairs of predictor variables. If the correlation coefficient, is exactly +1 or -1, this is called perfect multicollinearity, and one of the variables should be removed from the model if at all possible for the linear model to perform well.\n",
    "\n",
    "Desicion trees on the other hand, make no assumptions on relationships between features. It just constructs splits on single features that improves classification, based on an impurity measure like Gini or entropy. If features A, B are heavily correlated, no /little information can be gained from splitting on B after having split on A. So it would typically get ignored in favor of C.\n",
    "\n",
    "Of course a single decision tree is very vulnerable to overfitting, so one must either limit depth, prune heavily or preferly average many using an ensemble. Such problems get worse with many features and possibly also with co-variance but this problem occurs independently from multicolinearity.\n",
    "\n",
    "MLlib offers two correlation coefficient statitics: **pearson** and **spearman**. \n",
    "\n",
    "**Sources:**\n",
    "\n",
    " - https://datascience.stackexchange.com/questions/31402/multicollinearity-in-decision-tree\n",
    " - https://www.statisticshowto.datasciencecentral.com/multicollinearity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.stat import Correlation\n",
    "pearsonCorr = Correlation.corr(final_data, \"features\", \"pearson\").collect()[0][0]\n",
    "array = pearsonCorr.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      " \n",
      "0.8975232138340958\n",
      " \n",
      "0.974592615565013\n",
      "0.8975232138340958\n",
      " \n",
      "1.0\n",
      " \n",
      "0.9321897187331241\n",
      "0.974592615565013\n",
      " \n",
      "0.9321897187331241\n",
      " \n",
      "1.0\n",
      "0.014403796002494158\n",
      " \n",
      "0.03178967088561915\n",
      " \n",
      "0.04694401289092356\n"
     ]
    }
   ],
   "source": [
    "for item in array:\n",
    "    print(item[0])\n",
    "    print(\" \")\n",
    "    print(item[1])\n",
    "    print(\" \")\n",
    "    print(item[2])\n",
    "    # etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the first and second features are highly correlated, along with the 4th and 5th and 4th and 6th. We may want to consider removing one of the variables in each correlation pair if we decide to use a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split dataframe into training and evaluation (test) dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Let's train a Random Forest algorithm to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model\n",
    "regressor = LinearRegression()\n",
    "fitModel = regressor.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "We will use the Root Mean Squared Error as our evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which evaluator you want to use\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 82762.4\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = fitModel.transform(test)\n",
    "# Select (prediction, true label) and compute test error\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (without cross validation)\n",
    "\n",
    "**Recap from the content review lecture** <br>\n",
    "Linear regression is the most basic and commonly used predictive analysis. Regression estimates are used to describe data and to explain the relationship between one dependent variable and one or more independent variables.\n",
    "\n",
    "“Simple” linear regression means there is only one independent variable and “multiple” linear regression means there is more than one independent variables. \n",
    "\n",
    "Researchers often use a linear relationship to predict the (average) numerical value of Y for a given value of X using a straight line (called the regression line). If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y. In other words, you predict (the average) Y from X. Of course this is just for simple linear regression. As you add more variables, your graph will look more and more complex as it becomes a three-dimensional space. But this is the basic concept. Think back to your high school algebra class :) Your goal with this analysis is to find the line that has the least amount of error which is the sum of the distance between each one of the data points and the line.\n",
    "\n",
    "**PySpark Documention Link:** https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Model Summary without cross validation:\u001b[0m\n",
      " \n",
      "Intercept: -5000.035118262716\n",
      "\n",
      "+--------------+-------------------+\n",
      "|feature       |coeff              |\n",
      "+--------------+-------------------+\n",
      "|median_income |211485.0943816109  |\n",
      "|households    |121336.16194708904 |\n",
      "|total_bedrooms|-20159.75690472518 |\n",
      "|population    |-101511.65240711215|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "numIterations: 1\n",
      "objectiveHistory: (scaled loss + regularization) at each iteration \n",
      " [0.0]\n",
      "\n",
      "Training RMSE: 80914.195364\n",
      "Training r2: 0.506936\n",
      "\n",
      "Test RMSE: 82762.38075178427\n",
      "Test r2: 0.4900794482352304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "# first without cross val\n",
    "fitModel = regressor.fit(train)\n",
    "\n",
    "# Load the Summary\n",
    "trainingSummary = fitModel.summary\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\n",
    "    \"\\033[1m\" + \"Linear Regression Model Summary without cross validation:\" + \"\\033[0m\"\n",
    ")\n",
    "print(\" \")\n",
    "print(\"Intercept: %s\" % str(fitModel.intercept))\n",
    "print(\"\")\n",
    "coeff_array = fitModel.coefficients.toArray()\n",
    "coeff_scores = []\n",
    "for x in coeff_array:\n",
    "    coeff_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, coeff_scores), schema=[\"feature\", \"coeff\"]\n",
    ")\n",
    "print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\n",
    "    \"objectiveHistory: (scaled loss + regularization) at each iteration \\n %s\"\n",
    "    % str(trainingSummary.objectiveHistory)\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# Print the Errors\n",
    "print(\"Training RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"Training r2: %f\" % trainingSummary.r2)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Now load the test results\n",
    "test_results = fitModel.evaluate(test)\n",
    "\n",
    "# And print them\n",
    "print(\"Test RMSE: {}\".format(test_results.rootMeanSquaredError))\n",
    "print(\"Test r2: {}\".format(test_results.r2))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Model Summary WITH cross validation:\u001b[0m\n",
      " \n",
      "Coefficient Standard Errors: \n",
      "+--------------+------------------+\n",
      "|feature       |coeff std error   |\n",
      "+--------------+------------------+\n",
      "|households    |5151.850461695399 |\n",
      "|total_bedrooms|4212.479861208181 |\n",
      "|population    |2582.2658580028847|\n",
      "|median_income |1903.476116281847 |\n",
      "+--------------+------------------+\n",
      "\n",
      "None\n",
      " \n",
      "P Values: \n",
      "+--------------+---------------------+\n",
      "|feature       |P-Value              |\n",
      "+--------------+---------------------+\n",
      "|total_bedrooms|1.7263008291745763E-6|\n",
      "|median_income |0.0                  |\n",
      "|households    |0.0                  |\n",
      "|population    |0.0                  |\n",
      "+--------------+---------------------+\n",
      "\n",
      "None\n",
      " \n",
      "RMSE: 82762.35257818851\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "# Now train with cross val\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.maxIter, [10, 15])\n",
    "    .addGrid(regressor.regParam, [0.1, 0.01])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=regressor, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2\n",
    ")  # 3 is best practice\n",
    "\n",
    "print(\"\\033[1m\" + \"Linear Regression Model Summary WITH cross validation:\" + \"\\033[0m\")\n",
    "print(\" \")\n",
    "# Run cross validations\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Extract Best model\n",
    "LR_BestModel = fitModel.bestModel\n",
    "\n",
    "# Get Model Summary Statistics\n",
    "# ModelSummary = fitModel.bestModel.summary\n",
    "ModelSummary = LR_BestModel.summary\n",
    "print(\"Coefficient Standard Errors: \")\n",
    "coeff_ste = ModelSummary.coefficientStandardErrors\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, coeff_ste), schema=[\"feature\", \"coeff std error\"]\n",
    ")\n",
    "print(result.orderBy(result[\"coeff std error\"].desc()).show(truncate=False))\n",
    "print(\" \")\n",
    "print(\"P Values: \")\n",
    "# Then zip with input_columns list and create a df\n",
    "pvalues = ModelSummary.pValues\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, pvalues), schema=[\"feature\", \"P-Value\"]\n",
    ")\n",
    "print(result.orderBy(result[\"P-Value\"].desc()).show(truncate=False))\n",
    "print(\" \")\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "ModelPredictions = fitModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "test_results = evaluator.evaluate(ModelPredictions)\n",
    "print(\"RMSE:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "**Recap from the content review lecture** <br>\n",
    "Recall that Decision Trees break down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n",
    "\n",
    "Decision trees are most commonly used in classification exercises, but they can also used for regression problems as well. In a standard classification tree, the idea is to split the dataset based on homogeneity of data. Let’s say for example we have two variables: age and weight that predict if a person is going to sign up for a gym membership or not. In our training data if it showed that 90% of the people who are older than 40 signed up, we split the data here and age becomes a top node in the tree. We can almost say that this split has made the data \"90% pure\". \n",
    "\n",
    "In a regression tree the idea is this: since the target variable does not have classes, we fit a regression model to the target variable using each of the independent variables. Then for each independent variable, the data is split at several split points. At each split point, the \"error\" between the predicted value and the actual values is squared to get a \"Sum of Squared Errors (SSE)\". The split point errors across the variables are compared and the variable/point yielding the lowest SSE is chosen as the root node/split point. This process is recursively continued.\n",
    "\n",
    "The core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking. The ID3 algorithm can be used to construct a decision tree for regression by replacing Information Gain with Standard Deviation Reduction.\n",
    "\n",
    "\n",
    "**PySpark Documentation Link:** https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+--------------------+\n",
      "|feature       |score               |\n",
      "+--------------+--------------------+\n",
      "|median_income |0.9381680528249632  |\n",
      "|population    |0.03443881602895395 |\n",
      "|total_bedrooms|0.014203423297319504|\n",
      "|households    |0.0131897078487634  |\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      "82395.13316884734\n"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor()\n",
    "\n",
    "# Build your parameter grid\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30])\n",
    "    .addGrid(regressor.maxBins, [10, 20, 40])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=regressor,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
    "    numFolds=2,\n",
    ")  # 3 is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "DT_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees\n",
    "# in the ensemble The importance vector is normalized to sum to 1.\n",
    "print(\" \")\n",
    "print(\"\\033[1m\" + \" Feature Importances\" + \"\\033[0m\")\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "DT_featureImportances = DT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in DT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, imp_scores), schema=[\"feature\", \"score\"]\n",
    ")\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "**Recap from the content review lecture** <br>\n",
    "Recal from the Classification lectures that Random Forest is an ensemble method that predicts based on the majority of votes from each of the decision trees made. The idea is still the same here (with regression), it’s just that Standard Deviation Reduction techniques are used instead of Information Gain as we touched on in the Decision Tree explanation. \n",
    "\n",
    "**PySpark Documentation link:** https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+-------------------+\n",
      "|feature       |score              |\n",
      "+--------------+-------------------+\n",
      "|median_income |0.7759878148836421 |\n",
      "|population    |0.1067051325915351 |\n",
      "|households    |0.06419602339992711|\n",
      "|total_bedrooms|0.05311102912489574|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "76049.54534939601\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "    .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "    .addGrid(regressor.numTrees, [5, 20])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=regressor, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2\n",
    ")  # 3 is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "RF_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees\n",
    "# in the ensemble The importance vector is normalized to sum to 1.\n",
    "print(\" \")\n",
    "print(\"\\033[1m\" + \" Feature Importances\" + \"\\033[0m\")\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "RF_featureImportances = RF_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in RF_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, imp_scores), schema=[\"feature\", \"score\"]\n",
    ")\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Regressor\n",
    "\n",
    "**Recap from the content review lecture** <br>\n",
    "The same goes with gradient boosting (Standard Deviation Reduction techniques are used instead of Information Gain), but with more of a hierarchical approach. It combines the weak learners (binary splits) to strong prediction rules that allow a flexible partition of the feature space. The objective here, as is of any supervised learning algorithm, is to define a loss function and minimize it. So it basically tries a whole bunch of different ways to analyze your data and returns the best one. \n",
    "\n",
    "**PySpark Documentation link:** https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+-------------------+\n",
      "|feature       |score              |\n",
      "+--------------+-------------------+\n",
      "|population    |0.34139074009553433|\n",
      "|households    |0.2634584398185997 |\n",
      "|median_income |0.25480591149958576|\n",
      "|total_bedrooms|0.14034490858628018|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "76179.30283061681\n"
     ]
    }
   ],
   "source": [
    "regressor = GBTRegressor()\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "    .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=regressor, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2\n",
    ")  # 3 is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "GBT_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees\n",
    "# in the ensemble The importance vector is normalized to sum to 1.\n",
    "print(\" \")\n",
    "print(\"\\033[1m\" + \" Feature Importances\" + \"\\033[0m\")\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "GBT_featureImportances = GBT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in GBT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(\n",
    "    zip(input_columns, imp_scores), schema=[\"feature\", \"score\"]\n",
    ")\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take aways\n",
    "\n",
    "We can see from the output above that the GBTRegressor way out performed the other models judging from the RMSE (compare Training RMSE to all others). No surprise there. \n",
    "\n",
    "We also notice that the most of the P Values for the logistic regression model look pretty strong except for the second and third variables which we should consider removing if we go ahead with linear regression model. \n",
    "\n",
    "As for the tree models, it looks like population was the best feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how your predictions were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+\n",
      "|            features|   label|        prediction|\n",
      "+--------------------+--------+------------------+\n",
      "|[4.86753445045558...|452600.0| 390973.9178897271|\n",
      "|[7.00940893270863...|358500.0|472535.55636457296|\n",
      "|[5.25227342804663...|352100.0| 436478.1297779642|\n",
      "|[5.46383180502561...|341300.0|302776.08106375416|\n",
      "|[5.63835466933374...|342200.0|252647.82487356887|\n",
      "|[5.36597601502185...|269700.0| 212649.8129541716|\n",
      "|[6.19440539110467...|299200.0|242714.38461878544|\n",
      "|[6.53378883793334...|241400.0|259129.75922262663|\n",
      "|[6.50128967054038...|226700.0|155049.26226590684|\n",
      "|[6.56244409369372...|261100.0| 246150.8664893893|\n",
      "+--------------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = GBT_BestModel.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save your model you can simply..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this path will create the folder\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = str(datetime.now())  # return local time\n",
    "path = \"Models/GBT_Model_\" + timestamp\n",
    "GBT_BestModel.save(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
