{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging MLflow to better track and manage your results\n",
    "\n",
    "MLflow is organized into four components: **Tracking**, **Projects**, **Models**, and **Model Registry**. You can use each of these components on their own—for example, maybe you want to export models in MLflow’s model format without using Tracking or Projects—but they are also designed to work well together. So this notebook will focus on only the **Tracking** component within the PySpark environment. \n",
    "\n",
    "### Why is tracking useful/important?\n",
    "\n",
    "Machine learning typically requires experimenting with a diverse set of hyperparameter tuning techniques, data preparation steps, and algorithms to build a model that maximizes some target metric. Given this complexity, building a machine learning model can therefore be challenging for a couple of reasons:\n",
    "\n",
    "1. **It’s difficult to keep track of experiments.** When you are just working with files on your laptop, or with an interactive notebook, how do you tell which data, code and parameters went into getting a particular result?\n",
    "2. **It’s difficult to reproduce code.** Even if you have meticulously tracked the code versions and parameters, you need to capture the whole environment (for example, library dependencies) to get the same result again. This is especially challenging if you want another data scientist to use your code, or if you want to run the same code at scale on another platform (for example, in the cloud).\n",
    "\n",
    "### Solution that MLflow Tracking provides\n",
    "\n",
    "MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code and for later visualizing the results. You can use MLflow Tracking in any environment (for example, a standalone script or a notebook) to log results to local files or to a server, then compare multiple runs.\n",
    "\n",
    "### How to install MLflow\n",
    "\n",
    "You simply install MLflow by running *\"pip install mlflow\"* via the command line. Please reference the Quick Start Guide here for more details: https://mlflow.org/docs/latest/quickstart.html\n",
    "\n",
    "### Viewing the Tracking MLflow UI\n",
    "\n",
    "By default, wherever you run your program (Jupyter Notebook in this case), the tracking API writes data into files into a local ./mlruns directory. First you need to open your mlflow intance via the command line (cd into the folder where this notebook is stored). You can then run MLflow’s Tracking UI: http://localhost:5000/#/\n",
    "\n",
    "### How to cd into a folder\n",
    "\n",
    " - **Mac**: https://macpaw.com/how-to/use-terminal-on-mac\n",
    " - **Windows**: https://www.minitool.com/news/how-to-change-directory-in-cmd.html\n",
    " \n",
    "### Getting started\n",
    "\n",
    "Since we using PySpark for this example, first we need to create a Spark Session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ClassW_MLlib</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10eac0350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark  # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"ClassW_MLlib\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark\n",
    "# Click the hyperlinked \"Spark UI\" link to view details about your Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in functions we will need\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's read our dataset in for this notebook \n",
    "\n",
    "### Data Set Name: Autistic Spectrum Disorder Screening Data for Adult\n",
    "Autistic Spectrum Disorder (ASD) is a neurodevelopment condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis. The rapid growth in the number of ASD cases worldwide necessitates datasets related to behaviour traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilised for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioural features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behaviour science.\n",
    "\n",
    "### Source: \n",
    "https://www.kaggle.com/faizunnabi/autism-screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Datasets/autism-screening-for-toddlers/\"\n",
    "df = spark.read.csv(\n",
    "    path + \"Toddler Autism dataset July 2018.csv\", inferSchema=True, header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_No</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>Age_Mons</th>\n",
       "      <th>Qchat-10-Score</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Jaundice</th>\n",
       "      <th>Family_mem_with_ASD</th>\n",
       "      <th>Who completed the test</th>\n",
       "      <th>Class/ASD Traits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>White European</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>m</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>f</td>\n",
       "      <td>White European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>m</td>\n",
       "      <td>black</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Mons  Qchat-10-Score  \\\n",
       "0        1   0   0   0   0   0   0   1   1   0    1        28               3   \n",
       "1        2   1   1   0   0   0   1   1   0   0    0        36               4   \n",
       "2        3   1   0   0   0   0   0   1   1   0    1        36               4   \n",
       "3        4   1   1   1   1   1   1   1   1   1    1        24              10   \n",
       "4        5   1   1   0   1   1   1   1   1   1    1        20               9   \n",
       "5        6   1   1   0   0   1   1   1   1   1    1        21               8   \n",
       "\n",
       "  Sex       Ethnicity Jaundice Family_mem_with_ASD Who completed the test  \\\n",
       "0   f  middle eastern      yes                  no          family member   \n",
       "1   m  White European      yes                  no          family member   \n",
       "2   m  middle eastern      yes                  no          family member   \n",
       "3   m        Hispanic       no                  no          family member   \n",
       "4   f  White European       no                 yes          family member   \n",
       "5   m           black       no                  no          family member   \n",
       "\n",
       "  Class/ASD Traits   \n",
       "0                No  \n",
       "1               Yes  \n",
       "2               Yes  \n",
       "3               Yes  \n",
       "4               Yes  \n",
       "5               Yes  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Case_No: integer (nullable = true)\n",
      " |-- A1: integer (nullable = true)\n",
      " |-- A2: integer (nullable = true)\n",
      " |-- A3: integer (nullable = true)\n",
      " |-- A4: integer (nullable = true)\n",
      " |-- A5: integer (nullable = true)\n",
      " |-- A6: integer (nullable = true)\n",
      " |-- A7: integer (nullable = true)\n",
      " |-- A8: integer (nullable = true)\n",
      " |-- A9: integer (nullable = true)\n",
      " |-- A10: integer (nullable = true)\n",
      " |-- Age_Mons: integer (nullable = true)\n",
      " |-- Qchat-10-Score: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- Jaundice: string (nullable = true)\n",
      " |-- Family_mem_with_ASD: string (nullable = true)\n",
      " |-- Who completed the test: string (nullable = true)\n",
      " |-- Class/ASD Traits : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many classes do we have and what is their distribution?\n",
    "\n",
    "#### Number of classes:\n",
    "\n",
    "You'll want to know how many classes you have so you know whether you should use binary or multi-class classification methods. Each has it's own validation methods. \n",
    "\n",
    "#### Class distribution:\n",
    "It's important to check for class imbalance in your dependent variable for classification tasks. If there are extremley under or over represented classes, the accuracy of your model predictions might suffer as a result of your model essentially being biased. \n",
    "\n",
    "If you see class imbalance, one common way to correct this would be boot strapping or resampling your dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  326|\n",
      "|              Yes|  728|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Class/ASD Traits \").count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data \n",
    "\n",
    "#### Ensure your dependent variable is zero indexed\n",
    "MLlib requires a zero indexed variable for the dependent variable (luckily Pyspark has a built in function for this). We will also rename it so this code is reusable. \n",
    "\n",
    "#### Vectorize your input variables\n",
    "MLlib requires all input columns of your dataframe to be vectorized.\n",
    "\n",
    "#### Convert string data types to numeric\n",
    "MLlib requires this.\n",
    "\n",
    "#### Treat for skewness and outliers\n",
    "This is best practice, but optional. You can also add on other transformation methods here if you want depending on your use case. But this will give you some ideas on how to accomplish these types of tasks. \n",
    "\n",
    "#### Check for negative values in the df\n",
    "If you want to use the Naive Bayes classifier you have you rescale any variables that have negative values in them or else you will receive an error message here. This step is of course optional.\n",
    "\n",
    "#### Split into training and validation (test) dataframes\n",
    "This also best practice. It's a good way to test your model's ability to maintain it's performance on a new dataset. If you see the performance drop during testing, you will know your model needs improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare values you will need\n",
    "\n",
    "# col_list = [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\",\"Age_Mons\",\"Qchat-10-Score\",\"Sex\",\"Ethnicity\",\"Jaundice\",\"Family_mem_with_ASD\",\"Who completed the test\"]\n",
    "# input_columns = col_list\n",
    "\n",
    "input_columns = df.columns  # Collect the column names as a list\n",
    "input_columns = input_columns[1:-1]  # keep only relevant columns: from column 1 to\n",
    "\n",
    "dependent_var = \"Class/ASD Traits \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change label (class variable) to string type to prep for reindexing\n",
    "# Pyspark is expecting a zero indexed integer for the label column.\n",
    "# Just in case our data is not in that format... we will treat it by using the StringIndexer built in method\n",
    "renamed = df.withColumn(\n",
    "    \"label_str\", df[dependent_var].cast(StringType())\n",
    ")  # Rename and change to string type\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"label_str\", outputCol=\"label\"\n",
    ")  # Pyspark is expecting the this naming convention\n",
    "indexed = indexer.fit(renamed).transform(renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string type data in the input column list to numeric\n",
    "# Otherwise the Algorithm will not be able to process it\n",
    "\n",
    "# Also we will use these lists later on\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    # First identify the string vars in your input column list\n",
    "    if str(indexed.schema[column].dataType) == \"StringType\":\n",
    "        # Set up your String Indexer function\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column + \"_num\")\n",
    "        # Then call on the indexer you created here\n",
    "        indexed = indexer.fit(indexed).transform(indexed)\n",
    "        # Rename the column to a new name so you can disinguish it from the original\n",
    "        new_col_name = column + \"_num\"\n",
    "        # Add the new column name to the string inputs list\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        # If no change was needed, take no action\n",
    "        # And add the numeric var to the num list\n",
    "        numeric_inputs.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat for skewness\n",
    "# Flooring and capping\n",
    "# Plus if right skew take the log +1\n",
    "# if left skew do exp transformation\n",
    "# This is best practice\n",
    "\n",
    "# create empty dictionary d\n",
    "d = {}\n",
    "# Create a dictionary of quantiles from your numeric cols\n",
    "# I'm doing the top and bottom 1% but you can adjust if needed\n",
    "for col in numeric_inputs:\n",
    "    d[col] = indexed.approxQuantile(\n",
    "        col, [0.01, 0.99], 0.25\n",
    "    )  # if you want to make it go faster increase the last number\n",
    "\n",
    "# Now check for skewness for all numeric cols\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect()  # check for skewness\n",
    "    skew = skew[0][0]\n",
    "    # If skewness is found,\n",
    "    # This function will make the appropriate corrections\n",
    "    if skew > 1:  # If right skew, floor, cap and log(x+1)\n",
    "        indexed = indexed.withColumn(\n",
    "            col,\n",
    "            log(\n",
    "                when(df[col] < d[col][0], d[col][0])\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\n",
    "                .otherwise(indexed[col])\n",
    "                + 1\n",
    "            ).alias(col),\n",
    "        )\n",
    "        print(\n",
    "            col + \" has been treated for positive (right) skewness. (skew =)\", skew, \")\"\n",
    "        )\n",
    "    elif skew < -1:  # If left skew floor, cap and exp(x)\n",
    "        indexed = indexed.withColumn(\n",
    "            col,\n",
    "            exp(\n",
    "                when(df[col] < d[col][0], d[col][0])\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\n",
    "                .otherwise(indexed[col])\n",
    "            ).alias(col),\n",
    "        )\n",
    "        print(\n",
    "            col + \" has been treated for negative (left) skewness. (skew =\", skew, \")\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No negative values were found in your dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Now check for negative values in the dataframe.\n",
    "# Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used.\n",
    "# Note: we only need to check the numeric input values since anything that is indexed won't have negative values\n",
    "\n",
    "# Calculate the mins for all columns in the df\n",
    "minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs])\n",
    "# Create an array for all mins and select only the input cols\n",
    "min_array = minimums.select(array(numeric_inputs).alias(\"mins\"))\n",
    "# Collect golobal min as Python object\n",
    "df_minimum = min_array.select(array_min(min_array.mins)).collect()\n",
    "# Slice to get the number itself\n",
    "df_minimum = df_minimum[0][0]\n",
    "\n",
    "# If there are ANY negative vals found in the df, print a warning message\n",
    "if df_minimum < 0:\n",
    "    print(\n",
    "        \"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No negative values were found in your dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we correct for negative values that may have been found above,\n",
    "# We need to vectorize our df\n",
    "# becauase the function that we use to make that correction requires a vector.\n",
    "# Now create your final features list\n",
    "features_list = numeric_inputs + string_inputs\n",
    "# Create your vector assembler object\n",
    "assembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")\n",
    "# And call on the vector assembler to transform your dataframe\n",
    "output = assembler.transform(indexed).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1000.000000]\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1000.0,1000.0,0....|\n",
      "|  0.0|[1000.0,0.0,0.0,0...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  0.0|[1000.0,1000.0,0....|\n",
      "|  0.0|[1000.0,1000.0,0....|\n",
      "|  0.0|[1000.0,0.0,0.0,1...|\n",
      "|  0.0|[0.0,1000.0,0.0,0...|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  0.0|[1000.0,0.0,0.0,1...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|[1000.0,0.0,0.0,0...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the mix max scaler object\n",
    "# This is what will correct for negative values\n",
    "# I like to use a high range like 1,000\n",
    "#     because I only see one decimal place in the final_data.show() call\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", min=0, max=1000)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_data = scalerModel.transform(output)\n",
    "final_data = scaled_data.select(\"label\", \"scaledFeatures\")\n",
    "final_data = final_data.withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Test and Training datasets\n",
    "\n",
    "Now we can split into test and training datasets using whatever random split method we want. I will use 70/30 split but you can use your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40\n",
    "train_val = 0.7\n",
    "test_val = 1 - train_val\n",
    "train, test = final_data.randomSplit([train_val, test_val], seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Section\n",
    "\n",
    "Now that we have our data cleaned and vectorized we are ready to feed it into our training algorithms! As we went over in the Intro to Machine Learning lecture, the building blocks of a supervised ML application consist of some data for the model to \"learn\" from. Once there is data made available, then the person building the model must decide what the apprpriate dependent and independent variables are. Then they decide which algorithms to test, and compare the performance results of each model to each other before deciding which one to select. \n",
    "\n",
    "This process usually requires several trails until a decision is reached and diligent note-taking which can be tracked by MLflow. This is where we start using using it so buckle up! You can access the UI like this:\n",
    "\n",
    "1. Open your terminal\n",
    "2. Navigate to the folder where this notebook is stored using the \"cd\" command (eg. \"cd foldername)\n",
    "3. Then run this command \"mlflow ui\"\n",
    "4. Once it's done, navigate to this web address in your browser \"http://localhost:5000/#/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dependencies\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Mlflow libaries\n",
    "import mlflow\n",
    "from mlflow import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'Experiment-3' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "# Set experiment\n",
    "# This will actually automatically create one if the one you call on doesn't exist\n",
    "mlflow.set_experiment(experiment_name=\"Experiment-3\")\n",
    "\n",
    "# set up your client\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a run and attach it to the experiment you just created\n",
    "experiments = client.list_experiments()  # returns a list of mlflow.entities.Experiment\n",
    "\n",
    "experiment_name = \"Experiment-3\"\n",
    "\n",
    "\n",
    "def create_run(experiment_name):\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "    for x in experiments:\n",
    "        if experiment_name in x.name:\n",
    "            #             print(experiment_name)\n",
    "            #             print(x)\n",
    "            experiment_index = experiments.index(x)\n",
    "            run = client.create_run(\n",
    "                experiments[experiment_index].experiment_id\n",
    "            )  # returns mlflow.entities.Run\n",
    "            #             print(run)\n",
    "            return run\n",
    "\n",
    "\n",
    "# Example run command\n",
    "# run = create_run('Experiment-3')\n",
    "# run = create_run(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the functionality here\n",
    "run = create_run(\"Experiment-3\")\n",
    "\n",
    "# Add tag to a run\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", \"Gradient Boosted Tree\")\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", 908)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", 0.7)\n",
    "\n",
    "# Add params and metrics to a run\n",
    "client.log_param(run.info.run_id, \"Max Depth\", 90)\n",
    "client.log_param(run.info.run_id, \"Max Bins\", 50)\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", 0.87)\n",
    "\n",
    "# Terminate the client\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our classification and evaluation objects\n",
    "Bin_evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"prediction\"\n",
    ")  # labelCol='label'\n",
    "MC_evaluator = MulticlassClassificationEvaluator(\n",
    "    metricName=\"accuracy\"\n",
    ")  # redictionCol=\"prediction\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression without Cross Validation\n",
    "\n",
    "**Review**\n",
    "The Logistic Regression Algorithm, also known as \"Logit\", is used to estimate (guess) the probability (a number between 0 and 1) of an event occurring having been given some previous data to “learn” from. It works with either binary or multinomial (more than 2 categories) data and uses logistic function (ie. log) to find a model that fits with the data points.\n",
    "\n",
    "**Example**\n",
    "You may want to predict the likelihood of a student passing or failing an exam based on a set of biographical factors. The model you create will provide a probability (i.e a number between 0 and 1) that you can use to determine the likelihood of each student passing.\n",
    "\n",
    "PySpark Documentation Link: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 1.0\n",
      "Accuracy: 100.00 %\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Create a run\n",
    "run = create_run(experiment_name)\n",
    "\n",
    "# Simplist Method\n",
    "classifier = LogisticRegression()\n",
    "fitModel = classifier.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "predictionAndLabels = fitModel.transform(test)\n",
    "# predictionAndLabels = predictionAndLabels.predictions.select('label','prediction')\n",
    "auc = Bin_evaluator.evaluate(predictionAndLabels)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "print(\n",
    "    \"Accuracy: {0:.2f}\".format(accuracy), \"%\"\n",
    ")  #     print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\" \")\n",
    "\n",
    "# Log metric to MLflow\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Extract params of Best Model\n",
    "paramMap = fitModel.extractParamMap()\n",
    "\n",
    "# Log parameters to the client\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxIter\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Iter\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if \"regParam\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Reg Param\", val)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Cross Validation\n",
    "\n",
    "Spark also has a built-in funciton called the CrossValidator to conduct cross validation which begins by splitting the training dataset into a set of \"folds\" which are used as separate training and test datasets. For example, with k=5 folds, CrossValidator will generate 5 different (training, test) dataset pairs, each of which uses 4/5 of the data for training and 1/5 for testing. To evaluate a particular Parameter (specified in the paramgrid), CrossValidator computes the average evaluation metric for the 5 Models produced by fitting the Estimator on the 5 different (training, test) dataset pairs and tells you which model performed the best once it is finished. \n",
    "\n",
    "After identifying the best ParamMap (more details can be found in the documentation link above), CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset.\n",
    "\n",
    "**MaxIter:** <br>\n",
    "The maximum number of iterations to use. There is no clear formula for setting the optimum iteration number, but you can figure out this issue by an iterative process by initializing the iteration number by a small number like 100 and then increase it linearly. This process will be repeated until the MSE of the test does not decrease and even may increase. The below link describes well:\n",
    "https://www.quora.com/What-will-happen-if-I-train-my-neural-networks-with-too-much-iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [7.492597351937949]\n",
      "Coefficients: \n",
      "DenseMatrix([[-0.00156348, -0.00175451, -0.00125897, -0.00160323, -0.00157764,\n",
      "              -0.00160236, -0.00165045, -0.00173005, -0.00188974, -0.00139463,\n",
      "              -0.00021757, -0.00458864,  0.00014458, -0.00051631, -0.0005253 ,\n",
      "               0.00031766, -0.00162428]])\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "run = create_run(experiment_name)\n",
    "\n",
    "# This method uses cross validation and allows for hyperparamter tuning via grid searching\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Set up your parameter grid for the cross validator to consudt hyperparameter tuning\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(classifier.regParam, [0.1, 0.01])\n",
    "    .addGrid(classifier.maxIter, [10, 15, 20])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(),\n",
    "    numFolds=2,\n",
    ")  # 3 + is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Collect the best model and\n",
    "# print the coefficient matrix\n",
    "# These values should be compared relative to eachother\n",
    "# And intercepts can be prepared to other models\n",
    "BestModel = fitModel.bestModel\n",
    "print(\"Intercept: \" + str(BestModel.interceptVector))\n",
    "print(\"Coefficients: \\n\" + str(BestModel.coefficientMatrix))\n",
    "\n",
    "# Generate predictions\n",
    "# fitModel automatically uses the best model\n",
    "# so we don't need to use BestModel here\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# Now print the accuract rate of the model or AUC for a binary classifier\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", seed)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", train_val)\n",
    "\n",
    "# Log Model (can't do this to the client)\n",
    "# mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Extract params of Best Model\n",
    "paramMap = BestModel.extractParamMap()\n",
    "\n",
    "# Log parameters to the client\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxIter\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Iter\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if \"regParam\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Reg Param\", val)\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  LogisticRegression_89e693d592db__aggregationDepth :  2\n",
      "Key:  LogisticRegression_89e693d592db__elasticNetParam :  0.0\n",
      "Key:  LogisticRegression_89e693d592db__family :  auto\n",
      "Key:  LogisticRegression_89e693d592db__featuresCol :  features\n",
      "Key:  LogisticRegression_89e693d592db__fitIntercept :  True\n",
      "Key:  LogisticRegression_89e693d592db__labelCol :  label\n",
      "Key:  LogisticRegression_89e693d592db__maxIter :  20\n",
      "Key:  LogisticRegression_89e693d592db__predictionCol :  prediction\n",
      "Key:  LogisticRegression_89e693d592db__probabilityCol :  probability\n",
      "Key:  LogisticRegression_89e693d592db__rawPredictionCol :  rawPrediction\n",
      "Key:  LogisticRegression_89e693d592db__regParam :  0.0\n",
      "Key:  LogisticRegression_89e693d592db__standardization :  True\n",
      "Key:  LogisticRegression_89e693d592db__threshold :  0.5\n",
      "Key:  LogisticRegression_89e693d592db__tol :  1e-06\n"
     ]
    }
   ],
   "source": [
    "# To see a list of all available parameters\n",
    "for key, val in paramMap.items():\n",
    "    print(\"Key: \", key, \": \", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs. Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mIntercept: \u001b[0m -7.491888766919087 \u001b[1m\n",
      "Coefficients:\u001b[0m [0.0015619251060345694,0.0017476643739368146,0.0012676039959356592,0.0016071037812174273,0.0015806878350082855,0.0016030310084815566,0.0016454113996866868,0.0017346756018828899,0.0018827022321919109,0.0014000074782752057,0.00022041718443727376,0.00459034449017726,-0.00014342946859533873,0.0004986005045772529,0.0005321415098356268,-0.0003421958025736358,0.001514347036542753]\n",
      "\u001b[1mIntercept: \u001b[0m 7.491888766973003 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.0015619251059513155,-0.001747664373980319,-0.0012676039958683825,-0.001607103781323025,-0.0015806878350425604,-0.0016030310085631977,-0.0016454113998947484,-0.0017346756019040936,-0.001882702232215806,-0.001400007478286646,-0.0002204171843189781,-0.004590344490282593,0.00014342946855002954,-0.0004986005044933312,-0.0005321415097465272,0.0003421958026599253,-0.001514347036121395]\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Create a new run\n",
    "run = create_run(experiment_name)\n",
    "\n",
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression()\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "classifier = OneVsRest(classifier=lr)\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()\n",
    "# Cross Validator requires the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(),\n",
    "    numFolds=2,\n",
    ")  # 3 is best practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Print the Coefficients\n",
    "# First we need to extract the best model from fit model\n",
    "\n",
    "# Get Best Model\n",
    "BestModel = fitModel.bestModel\n",
    "# Extract list of binary models\n",
    "models = BestModel.models\n",
    "for model in models:\n",
    "    print(\n",
    "        \"\\033[1m\" + \"Intercept: \" + \"\\033[0m\",\n",
    "        model.intercept,\n",
    "        \"\\033[1m\" + \"\\nCoefficients:\" + \"\\033[0m\",\n",
    "        model.coefficients,\n",
    "    )\n",
    "\n",
    "# Now generate predictions on test dataset\n",
    "predictions = fitModel.transform(test)\n",
    "# And calculate the accuracy score\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "# And print\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", seed)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", train_val)\n",
    "\n",
    "# Log Model (can't do this to the client)\n",
    "# mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Extract params of Best Model\n",
    "paramMap = BestModel.extractParamMap()\n",
    "\n",
    "# Log parameters to the client\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxIter\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Iter\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if \"regParam\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Reg Param\", 5)\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Classifier\n",
    "\n",
    "*Neural Network* <br>\n",
    "\n",
    "**Recap from the lecture** <br>\n",
    "A multilayer perceptron (MLP) is a class of feedforward artificial neural network. It consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n",
    "\n",
    "#### Common Hyper Parameters:\n",
    "\n",
    "**MaxIter:** <br>\n",
    "The maximum number of iterations to use. There is no clear formula for setting the optimum iteration number, but you can figure out this issue by an iterative process by initializing the iteration number by a small number like 100 and then increase it linearly. This process will be repeated until the MSE of the test does not decrease and even may increase. The below link describes well:\n",
    "https://www.quora.com/What-will-happen-if-I-train-my-neural-networks-with-too-much-iteration\n",
    "\n",
    "**Layers:** <br>\n",
    "Spark requires that the input layer equals the number of features in the dataset, the hidden layer might be one or two more than that (flexible), and the output layer has to be equal to the number of classes. Here's a great article to learn more about how to play around with the hidden layers: https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e\n",
    "\n",
    "**Block size:** <br>\n",
    "Block size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128\n",
    "\n",
    "**Seed:** <br>\n",
    "A random seed. Set this value if you need your results to be reproducible across repeated calls (highly recommdended).\n",
    "\n",
    "**Weights**: *printed for us below along with accuracy rate* <br> \n",
    "Each hidden neuron added will increase the number of weights, thus it is recommended to use the least number of hidden neurons that accomplish the task. Using more hidden neurons than required will add more complexity.\n",
    "\n",
    "**PySpark Documentation link:** <br> \n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel Weights: \u001b[0m 683\n",
      "90.84967320261438\n"
     ]
    }
   ],
   "source": [
    "# Create a new run\n",
    "run = create_run(experiment_name)\n",
    "\n",
    "# Count how many features you have\n",
    "features = final_data.select([\"features\"]).collect()\n",
    "features_count = len(features[0][0])\n",
    "# Then use this number to specify the layers according to best practice\n",
    "layers = [features_count, features_count + 1, features_count, classes]\n",
    "# Instaniate the classifier\n",
    "classifier = MultilayerPerceptronClassifier(\n",
    "    maxIter=100, layers=layers, blockSize=128, seed=1234\n",
    ")\n",
    "\n",
    "# Fit the model (this classifier doesn't have a cross validator option)\n",
    "fitModel = classifier.fit(train)\n",
    "\n",
    "# Print the model Weights\n",
    "print(\"\\033[1m\" + \"Model Weights: \" + \"\\033[0m\", fitModel.weights.size)\n",
    "\n",
    "# Generate predictions on test dataframe\n",
    "predictions = fitModel.transform(test)\n",
    "# Calculate accuracy score\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "# Print accuracy score\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", seed)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", train_val)\n",
    "client.set_tag(run.info.run_id, \"Model Weight\", fitModel.weights.size)\n",
    "\n",
    "# # Log Model (can't do this to the client)\n",
    "# # mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "**Recap from the lecture:**\n",
    "The Naive Bayes Classifier is a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature. \n",
    "\n",
    "So for example, a fruit may be considered to be an apple if it is red, round, and about 3″ in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features. Features, however, aren’t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it’s labeled “naive”.\n",
    "\n",
    "**Assumptions:**\n",
    " - Independence between every pair of features\n",
    " - Feature values are nonnegative (which is why we checked earlier)\n",
    "\n",
    "**Hyper Parameters:**\n",
    "\n",
    " - **smoothing** = It is problematic when a frequency-based probability is zero, because it will wipe out all the information in the other probabilities, and we need to find a solution for this. A solution would be Laplace smoothing , which is a technique for smoothing categorical data. In PySpark, this number needs to be be >= 0, default is 1.0'. Also here is a great article that defines smoothing in more detail: https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    " - **thresholds** = Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. The default value is none. \n",
    " - **weightCol** = If you have a weight column you would enter the name of the column here. If this is not set or empty, we treat all instance weights as 1.0. To learn more about the theory behind this, here is a good paper: http://pami.uwaterloo.ca/~khoury/ece457f07/Zhang2004.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.60130718954248\n"
     ]
    }
   ],
   "source": [
    "# Create a new run\n",
    "run = create_run(experiment_name)\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "classifier = NaiveBayes()\n",
    "paramGrid = (\n",
    "    ParamGridBuilder().addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]).build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(),\n",
    "    numFolds=2,\n",
    ")  # 3 + is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", seed)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", train_val)\n",
    "\n",
    "# Log Model (can't do this to the client)\n",
    "# mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Extract params of Best Model\n",
    "# Get Best Model\n",
    "BestModel = fitModel.bestModel\n",
    "paramMap = BestModel.extractParamMap()\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine\n",
    "\n",
    "**Recap from lecture:**\n",
    "Linear SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes, which is why you can only use it for binary classification. Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set. Intuitively, the further from the hyperplane our data points lie, the more confident we are that they have been correctly classified. We therefore want our data points to be as far away from the hyperplane as possible, while still being on the correct side of it. So when new testing data is added, whatever side of the hyperplane it lands will decide the class that we assign to it.\n",
    "\n",
    "**Interpretting the coefficients:**\n",
    "\n",
    "Each coefficients direction gives us the predicted class, so if you take the dot product of any point with the vector, you can tell on which side it is: if the dot product is positive, it belongs to the positive class, if it is negative it belongs to the negative class.\n",
    "\n",
    "You can even learn something about the importance of each feature. Let's say the svm would find only one feature useful for separating the data, then the hyperplane would be orthogonal to that axis. So, you could say that the absolute size of the coefficient relative to the other ones gives an indication of how important the feature was for the separation. \n",
    "\n",
    "**Hyper Parameters:** <br>\n",
    "\n",
    "**MaxIter:** <br>\n",
    "The maximum number of iterations to use. There is no clear formula for setting the optimum iteration number, but you can figure out this issue by an iterative process by initializing the iteration number by a small number like 100 and then increase it linearly. This process will be repeated until the MSE of the test does not decrease and even may increase. The below link describes well:\n",
    "https://www.quora.com/What-will-happen-if-I-train-my-neural-networks-with-too-much-iteration\n",
    "\n",
    "**regParam**: <br>\n",
    "The purpose of the regularizer is to encourage simple models and avoid overfitting. To learn more about this concept, here is an interesting article: https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
    "\n",
    "**PySpark Documentation link:** <br> https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Your good to go!\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]\n",
    "if classes > 2:\n",
    "    print(\n",
    "        \"LinearSVC cannot be used because PySpark currently only accepts binary classification data for this algorithm\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Your good to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "[-0.0004004187718846855,-0.0007999466788933074,-0.00020980540757841628,-0.00033661624616255836,-0.0005794255238525335,-0.0005096275936528429,-0.0003586112967212599,-0.0008362254956535799,-0.0007857927514483761,-0.00023763499184109818,0.0008153755671898964,-0.001594010760387467,0.00033870854896445827,-7.935329231477935e-05,-0.00023292166302618764,0.00013986560862956683,-0.0004519922206593421]\n",
      "98.0392156862745\n"
     ]
    }
   ],
   "source": [
    "# Create a new run\n",
    "run = create_run(experiment_name)\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "classifier = LinearSVC()\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(classifier.maxIter, [10, 15])\n",
    "    .addGrid(classifier.regParam, [0.1, 0.01])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(),\n",
    "    numFolds=2,\n",
    ")  # 3 + is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "BestModel = fitModel.bestModel\n",
    "\n",
    "print(\"\\033[1m\" + \" Coefficients\" + \"\\033[0m\")\n",
    "print(\"You should compares these relative to eachother\")\n",
    "print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
    "\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", seed)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", train_val)\n",
    "\n",
    "# Log Model (can't do this to the client)\n",
    "# mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Extract params of Best Model\n",
    "paramMap = BestModel.extractParamMap()\n",
    "\n",
    "# Log parameters to the client\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxIter\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Iter\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if \"regParam\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Reg Param\", 5)\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "**Recall from the lecture:**\n",
    "Decision Trees classifiers  are a supervised learning method used to classify a variable by learning from historical data that the model uses to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model. \n",
    "\n",
    "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n",
    "\n",
    "### Common Hyper Parameters\n",
    "\n",
    " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
    "     - **Continuous features:** For small datasets in single-machine implementations, the split candidates for each continuous feature are typically the unique values for the feature. Some implementations sort the feature values and then use the ordered unique values as split candidates for faster tree calculations.\n",
    "         Sorting feature values is expensive for large distributed datasets. This implementation computes an approximate set of split candidates by performing a quantile calculation over a sampled fraction of the data. The ordered splits create “bins” and the maximum number of such bins can be specified using the maxBins parameter.\n",
    "         Note that the number of bins cannot be greater than the number of instances N (a rare scenario since the default maxBins value is 32). The tree algorithm automatically reduces the number of bins if the condition is not satisfied.\n",
    "\n",
    "     - **Categorical features:** For a categorical feature with M possible values (categories), one could come up with 2 exp(M−1) −1 split candidates. For binary (0/1) classification and regression, we can reduce the number of split candidates to M−1 by ordering the categorical feature values by the average label. For example, for a binary classification problem with one categorical feature with three categories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical features are ordered as A, C, B. The two split candidates are A | C, B and A , C | B where | denotes the split.\n",
    "         In multiclass classification, all 2 exp(M−1) −1 possible splits are used whenever possible. When 2 exp(M−1) −1 is greater than the maxBins parameter, we use a (heuristic) method similar to the method used for binary classification and regression. The M categorical feature values are ordered by impurity, and the resulting M−1 split candidates are considered.\n",
    "         \n",
    " - **maxDepth** = The max_depth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
    "\n",
    "### Feature Importance Scores\n",
    "Scores add up to 1 accross all varaibles so the lowest score is the least imporant variable. \n",
    "\n",
    "\n",
    "### Extra Reading\n",
    "**How to tune a decision tree** <br>\n",
    "https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n",
    "\n",
    "**PySpark Documentation link:** <br> https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Scores (add up to 1)\n",
      "(17,[11],[1.0])\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Create a new run\n",
    "run = create_run(experiment_name)\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "classifier = DecisionTreeClassifier()\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()  #                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "    .addGrid(classifier.maxBins, [10, 20, 40, 80, 100])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(),\n",
    "    numFolds=2,\n",
    ")  # 3 + is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Collect and print feature importances\n",
    "BestModel = fitModel.bestModel\n",
    "print(\"Feature Importance Scores (add up to 1)\")\n",
    "featureImportances = BestModel.featureImportances.toArray()\n",
    "print(featureImportances)\n",
    "\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", seed)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", train_val)\n",
    "\n",
    "# Log Model (can't do this to the client)\n",
    "# mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Extract params of Best Model\n",
    "paramMap = BestModel.extractParamMap()\n",
    "\n",
    "# Log parameters to the client\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxDepth\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Depth\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxBins\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Bins\", 5)\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "**Recal from the lecture** <br>\n",
    "Suppose you have a training set with 6 classes, random forest may create three decision trees taking input of each subset like the example on the left. Finally, it predicts based on the majority of votes from each of the decision trees made. This works well because a single decision tree may be prone to a noise, but aggregate of many decision trees reduce the effect of noise giving more accurate results. The subsets in different decision trees created may overlap. \n",
    "\n",
    "\n",
    "### Common Hyper Parameters\n",
    "\n",
    " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
    "     - **Continuous features:** For small datasets in single-machine implementations, the split candidates for each continuous feature are typically the unique values for the feature. Some implementations sort the feature values and then use the ordered unique values as split candidates for faster tree calculations.\n",
    "         Sorting feature values is expensive for large distributed datasets. This implementation computes an approximate set of split candidates by performing a quantile calculation over a sampled fraction of the data. The ordered splits create “bins” and the maximum number of such bins can be specified using the maxBins parameter.\n",
    "         Note that the number of bins cannot be greater than the number of instances N (a rare scenario since the default maxBins value is 32). The tree algorithm automatically reduces the number of bins if the condition is not satisfied.\n",
    "\n",
    "     - **Categorical features:** For a categorical feature with M possible values (categories), one could come up with 2 exp(M−1) −1 split candidates. For binary (0/1) classification and regression, we can reduce the number of split candidates to M−1 by ordering the categorical feature values by the average label. For example, for a binary classification problem with one categorical feature with three categories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical features are ordered as A, C, B. The two split candidates are A | C, B and A , C | B where | denotes the split.\n",
    "         In multiclass classification, all 2 exp(M−1) −1 possible splits are used whenever possible. When 2 exp(M−1) −1 is greater than the maxBins parameter, we use a (heuristic) method similar to the method used for binary classification and regression. The M categorical feature values are ordered by impurity, and the resulting M−1 split candidates are considered.\n",
    "         \n",
    " - **maxDepth** = The maxDepth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
    "\n",
    "### Feature Importance Scores\n",
    "Scores add up to 1 accross all varaibles so the lowest score is the least imporant variable. \n",
    "\n",
    "PySpark Documentation link: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Scores (add up to 1)\n",
      "(17,[1,3,4,5,6,7,8,11],[0.017224440216344422,0.046165347175294885,0.11322498026865448,0.09906105026114233,0.04271763626851278,0.022736982295678707,0.142060995596038,0.5168085679183345])\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Create a new run\n",
    "run = create_run(experiment_name)\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "classifier = RandomForestClassifier()\n",
    "paramGrid = (\n",
    "    ParamGridBuilder().addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "    #                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "    #                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(),\n",
    "    numFolds=2,\n",
    ")  # 3 + is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Retrieve best model from cross val\n",
    "BestModel = fitModel.bestModel\n",
    "print(\"Feature Importance Scores (add up to 1)\")\n",
    "featureImportances = BestModel.featureImportances.toArray()\n",
    "print(featureImportances)\n",
    "\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", seed)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", train_val)\n",
    "\n",
    "# Log Model (can't do this to the client)\n",
    "# mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Extract params of Best Model\n",
    "paramMap = BestModel.extractParamMap()\n",
    "\n",
    "# Log parameters to the client\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxDepth\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Depth\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxBins\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Bins\", 5)\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Tree Classifier\n",
    "\n",
    "**Recall from the lecture**\n",
    "With gradient boosting, it’s more of a hierarchical approach. It combines the weak learners (binary splits) to strong prediction rules that allow a flexble partition of the feature space. The objective here, as is of any supervised learning algorithm, is to define a loss function and minimize it. \n",
    "\n",
    "### Common Hyper Parameters\n",
    "\n",
    " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
    "     - **Continuous features:** For small datasets in single-machine implementations, the split candidates for each continuous feature are typically the unique values for the feature. Some implementations sort the feature values and then use the ordered unique values as split candidates for faster tree calculations.\n",
    "         Sorting feature values is expensive for large distributed datasets. This implementation computes an approximate set of split candidates by performing a quantile calculation over a sampled fraction of the data. The ordered splits create “bins” and the maximum number of such bins can be specified using the maxBins parameter.\n",
    "         Note that the number of bins cannot be greater than the number of instances N (a rare scenario since the default maxBins value is 32). The tree algorithm automatically reduces the number of bins if the condition is not satisfied.\n",
    "\n",
    "     - **Categorical features:** For a categorical feature with M possible values (categories), one could come up with 2 exp(M−1) −1 split candidates. For binary (0/1) classification and regression, we can reduce the number of split candidates to M−1 by ordering the categorical feature values by the average label. For example, for a binary classification problem with one categorical feature with three categories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical features are ordered as A, C, B. The two split candidates are A | C, B and A , C | B where | denotes the split.\n",
    "         In multiclass classification, all 2 exp(M−1) −1 possible splits are used whenever possible. When 2 exp(M−1) −1 is greater than the maxBins parameter, we use a (heuristic) method similar to the method used for binary classification and regression. The M categorical feature values are ordered by impurity, and the resulting M−1 split candidates are considered.\n",
    "         \n",
    " - **maxDepth** = The maxDepth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
    "\n",
    "### Feature Importance Scores\n",
    "Scores add up to 1 accross all varaibles so the lowest score is the least imporant variable. \n",
    "\n",
    "PySpark Documentation link: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're good to go!\n"
     ]
    }
   ],
   "source": [
    "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]\n",
    "if classes > 2:\n",
    "    print(\n",
    "        \"GBTClassifier cannot be used because PySpark currently only accepts binary classification data for this algorithm\"\n",
    "    )\n",
    "else:\n",
    "    print(\"You're good to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Scores (add up to 1)\n",
      "(17,[0,1,2,3,6,7,9,10,11,13,16],[1.376952975262389e-19,3.648925384445331e-18,1.1391289706771316e-15,1.3154732332751593e-16,4.089550336529295e-17,5.793370974644572e-17,6.722079298824236e-17,3.636265198083231e-16,0.9999999999999913,1.452358788184794e-17,7.0537007220663794e-15])\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Create a new run\n",
    "run = create_run(experiment_name)\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "classifier = GBTClassifier()\n",
    "\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()  #                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "    #                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "    .addGrid(classifier.maxIter, [10, 15, 50]).build()\n",
    ")\n",
    "\n",
    "# Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(),\n",
    "    numFolds=2,\n",
    ")  # 3 + is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "BestModel = fitModel.bestModel\n",
    "print(\"Feature Importance Scores (add up to 1)\")\n",
    "featureImportances = BestModel.featureImportances.toArray()\n",
    "print(featureImportances)\n",
    "\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions)) * 100\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)\n",
    "client.set_tag(run.info.run_id, \"Random Seed\", seed)\n",
    "client.set_tag(run.info.run_id, \"Train Perct\", train_val)\n",
    "\n",
    "# Log Model (can't do this to the client)\n",
    "# mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Extract params of Best Model\n",
    "paramMap = BestModel.extractParamMap()\n",
    "\n",
    "# Log parameters to the client\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxDepth\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Depth\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if \"maxBins\" in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Bins\", 5)\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "\n",
    "# Set a runs status to finished (best practice)\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
